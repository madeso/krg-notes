/**
 * \defgroup actuation actuation / execution / action layer
 * @{
 */

///////////////////////////////////////////////////////////////////////////////////////////////////
// actuation / execution / action layer

enum class Result { invalid, active, failed, succeeded };
enum class StopReason { completed, interrupted };

/** Something a Agent can do.
 * 
 * Core of the actuation / execution / action layer.
 * 
 * Can be used for: AI, cinematics, scripting and testing.
 * 
 * Need 2 (or more) layers for the action system
 * 
 * 1. Sequential/primary (full body)
 *    * locomotion (move-to, idle, navlink, jump...)
 *    * reaction (hit, events...)
 *    * attack/interaction (punch, revive, smart object...)
 * 
 * 2. Parallel/secondary (upper body/overlay), run across multiple sequential actions
 *    * overlay actions (shooting, reloading, thow...)
 *    * focus/state actions (look-at, injured, scared...)
 * 
 * Examples:
 *  * Move the agent (locomotion, navlinkgs, reactions, smart objects) (with or without combat bool)
 *  * Playing animation(and handling side effecs)
 *  * Triggering audio (barks, reactions)
 * 
 * Bark has tiers:
 *  * Generic (I saw something)
 *  * Specific (I saw a light go out over there)
*/
struct Action
{
    virtual void start() = 0;
    virtual Result update() = 0;
    virtual void stop(StopReason reason) = 0; // query action to get the reason for failure (for robustness, debugging, logging)
};

struct MoveToAction : Action
{
    void start() override
    {
        // set up character gameplay & physics state
        // lock to navmesh
        // enable gravity?
        // setup animation state (locomotion state)
        // set up everything... assume nothing
    }

    Result update() override
    {
        if(false == has_path || path_invalidated)
        {
            if(false == try_find_path())
            {
                return Result::failed;
            }
        }

        if(moving_along_path)
        {
            // path following logic
            if(is_within_stopping_distance)
            {
                // trigger stop
            }
        }
        else if(is_stopping)
        {
            if(stop_animation_complete)
            {
                return Result::succeeded;
            }
        }
        else if(is_idle)
        {
            // trigger start
        }

        return Result::active;
    }

    void stop(StopReason) override
    {
        // release path handle
        // trigger stop if we're moving (for failer/interuption)
    }
};


/**@}*/
/**
 * \defgroup knowledge perception / knowledge
 * @{
 */


struct Agent
{
    Knowledge Knowledge;
    float knowledge_update_timer;

    Behavior* active_behavior;
    BehaviorGroup behaviors;
    BehaviorGroup high_priority; // (reactions, critical (falling, frozen...) )
    Behavior* default_behavior;

    void knowledge_update();
    void sensor_update();
};


/** A AI-only representation of the world
 */
struct World
{
    // agents
    // players
    // visual stim: explosions, dead bodies, blood, character action (reload, punch...)
    // audio stim (footsteps, gunshots, explosion, bark...)
    // enviromental/generic stims (last known position, breadcrumbs, alarms, light columes, aggro auras...)
    // smart objects/interactables/covers
    // global state/behavior modifiers

    // object state and history for certain objects (light switches...)

    // shared tactical state
    // shared information system
    SharedInformation shared_information;

    // transfer relevant state to agents

    // runs ai simulation and updates  (updates, sensor updates, scheduling)

    /** Update a agent state.
     * Run all updates for a agent on a single frame
     * Update order: sensor_update -> goal_generator -> behavior_selection
     */
    void update_agent(Agent* agent, float dt)
    {
        agent->knowledge_update_timer -= dt;
        if(agent->knowledge_update_timer <= 0.0f)
        {
            // knowledge_update each 100-150ms reflect current state of the game to agent knowledge
            // can be run slower than behavior_selection and it's ok if it runs on stale data
            agent->knowledge_update();
            agent->sensor_update();

            goal_generator();
        }

        behavior_selection(agent);
    }
};

/** A stimuli (AI event or AI markup) for agents to reac to.
 * Generated by fire and forget function "events" or by scripting or by turning on/enabling a area "event".
 * 
 * Instant stims can affect decision making: hit reaction/bullet hit and explosions for example.
 */
struct Stim
{
    // type

    /** The severity or scale of the stimuli.
     * Was it small, medium or large?
    */
    float severity_or_scale; // include both (differentiate between teacup falling near me and explosion far away)
    vec3 position;

    // lifetime (optional)
    // source agent/object id

    // limit to a location/area
    // lifetime: audio have a timer, visual expire after 1 frame
    // can be restricted to specfic users
};

/** What an agent knows of the world, personal/private to the agent
 * not a blackboard, use a more explicit structure.
 * 
 * Know their friends and notice if someone is missing? Contains current alert status? Don't go back down to idle.
 */
struct Knowledge
{
};

/** Sensors read from from ai world to knowledge
 * Switch sensors based on scripting or ai state:
 *  * Relaxed vs combat sensors
 *  * Noisy enviroment reduce audio range
 */
struct Sensor
{
};


/** Creates Knowledge for what the agent can see.
 * 
 * It has a set of local shapes for seeing:
 *  * long shape for distance vision cone+box < = so it can go wide fast but be long but not wide. Reuse frustrum?
 *  * shorter for peropheral vision
 *  * one for personal space
 * 
 * combat sensor: large 5 meter shape for "combat vision"
 * 
 * Checks all registred visual objects in the world agains the shapes.
 * Objects withing the shapes can be checked for visibility using raycasts (or use navmesh line of sight).
 * Raycast to multiple bones (head, chest, elbow, knee) -> any unblocked = "visible"
 * 
 * Difficulty determines detection threshold and increase rate.
 * Target stance = determines minimum of bones visible
 * 
 */
struct VisualSensor : Sensor
{
};

/** Creates Knowledge for what the agent can hear.
 * 
 * Enemies on screen hear better than enemies off-screen
 * 
 * A simple distance based sensor.
 * Closest point on navmesh has issues, best is to use 3d pathfinding but it might not be needed.
*/
struct AudioSensor : Sensor
{
};


/** Global shared state.
 * Useful for optimizing longer calculations and could be used to fake group behavior.
 * 
 * Example: stealth search
 * * :ook up search interactions (smart objects/interactables)
 * * Create a search zone and store status
 * * Multiple agents can join "the search" and flag objects as "searched"
 * * Zones are "localized" and agents can find existing or merge zones
 * 
 * A combat zone
 * * Share cover information
 * * Evaluate positions with cover and line of sight to player
 */
struct SharedInformation
{
};

/**@}*/

/**
 * \defgroup decisionmaking decision making
 * @{
 */

// Don't make it too complex. From a player perspective "pick the closest cover and shoot at player"
// might be comparable to "cover evaluation with a line of sight eval"

enum class ExecutionState { running, completed, failed };



/** A sequence of actions that achieve some goal.
 * 
 * Can be authored in code, with a behavior tree or a scripting language
 * 
 * 
 * Can be linear but can also be follow path and use smart objects at control waypoint.
 * 
 * Request behaviour via test harness.
 * 
 * A sequencer for the actuation layer:
 * 1. Request a new action
 * 2. Wait for action to complete
 * 3. Loop back to step 1.
 * 
 * Has config/settings that can control certain aspects (distance to move, speed custom precondition, fire rate, etc...)
 * 
 * Reuse behavior by creating new behavior from other behavior with different options.
 * 
 * Examples:
 *  * Move to X, play animation
 *  * Move to X while shooting, reloading/etc...
 *  * Play reaction animation Y and audio bark Z
 * 
 */
struct Behavior
{
    /** A unique name.
    */
    std::string name;

    virtual ExecutionState update() = 0;

    /** The behavior may or may not be interuptable.
     */
    virtual bool is_interupptable() = 0;
    
    /** Can this behavior be started?
     */
    bool is_valid_selection_option() const { return is_cooling_down() == false && are_starting_conditions_met(); }

    /** Is the behavior currently cooling down?
     */
    bool is_cooling_down() const;

    /** Are the coditions valid for this behavior to be started?
     * Pre-conditions are usually tied to goal, but not always. For example: Using a path requires a path.
     */
    virtual bool are_starting_conditions_met() const { return true; }

    /** Calculate a score only used for behavior selection when we have multiple options.
     * Dynamically calculated based on situation/settings/etc.
     */
    virtual float calculate_selection_score() const { return 1.0f; }
};


/** Used to share precondition across multiple behaviors.
 * Essentially a list of behaviours with shared preconditions and it can contain other groups.
 * It can be viewed as a folder in a file system with Behavior as files.
 * 
 * Examples
 *  * Attack(melee, ranged)
 *  * Combat (revive target, chase target, ATTACK)
 *  * Reactions(hit reaction, general reaction)
 *  * Search(investigate, search for target)
 * 
 * Other examples
 *  * Revive target
 *  * Use smart object
 */
struct BehaviorGroup
{
    /** A unique name
     */
    std::string name;

    std::vector<BehaviorGroup*> subgroups;
    std::vector<BehaviorGroup*> behaviours;

    /** Append matching behavior to a out variable.
    */
    void append_behaviors(std::vector<Behavior*>* selected_behaviors) const
    {
        if(are_preconditions_met() == false) { return; }

        for(const auto& group: subgroups)
        {
            group->append_behavior(selected_behaviors);
        }

        for(const auto& beh: behaviors)
        {
            if(beh->is_valid_selection_option())
            {
                selected_behaviors->emplace_back(beh);
            }
        }
    }

    virtual bool are_preconditions_met() const { return true; }
};

// behavior selection:

// basic
// sequence multiple behaviors
// behavior cooldown (I just did this, I don't want to do this again for X time)
// pre conditions
// reuse with different settings

// advanced:
// runtime disabling of existing behaviors
// runtime injection of new behaviors

// many agents
// manager centric (aka director, coordinator) (agents are puppets)
// agent centric (agents decide)
// managers are useful for dumb zombies

// group behavior:
//  covering fire:
//    agent 1
//      if others agents are nearby and cooldown available
//      bark: "cover me"
//      spawn stim volume needs-cover-fire
//      execute a move to
//    agent 2
//      if in needs-cover-fire and needs to shoot (stim zone influence)
//      bark: "covering!"
//      already-shooting? just bark!
//
// hear a noise:
//     bark: heard-a-noise
//     spawn a volume to get a response (I agree, lets check it out...)
//
// mobile cover:
//  attach smart object/mobile cover point to a moving object (or another player with riotshield)
//
// social interactions:
//  smart objects: multiple users of same object (fireplace) that switch actions
//  syncronised animation/paired interaction with dialog and conversation


// ----------------------------------------------------------------------------

// are there any events I want to react to
// are there any objects that I could use
// are there any threats nearby
// are there available covers that I can use


/** Piece of data that defines a "high level thing I can do/use".
 *  Complex/compound knowledge entry: here is all the things you can do (not have to!) + all information to do it.
 * 
 *  Only specify what is available/possible (ie see enemy, not fight or flee).
 *  It can have hints: there are 5 enemies but the aggro system says target this one or other agents are targeting the other.
 * 
 *  Generated by generators ("advanced sensors").
 *  Why? optimization, debugging (decision is separate from knowledge)
 * 
 * Examples:
 *  * follow a patrol (patrol list)
 *  * use a smart object (smart object list)
 *  * look at point of interest
 *  * react to X (list...)
 *  * fight and enemy
 *  * shoot an explosive barrel
 *  * investigate (list...)
 *  
 *  More examples:
 * * combat goal: listed of every enemy, everyt combat interactable...
 * * investigation: points of interest to search within 10 meters
 * * if we had a new gunshot audio stim, we create a react goal with that stim linked
 * * if an agent can see enemies, we create a combat goal with all enemy agents
 * * if there are available covers near the agent we create a use cover goal with those covers linked
 */
struct Goal
{
    // contains bunch of custom data
    // might link to other Knowledge entries

    float lifetime;
    bool completed = false;

    bool should_be_removed() const { return completed || lifetime <= 0.0f; }
};


/** Determines what a agent could do?
 * 
 * Convert world/agent Knowledge into goals.
 * 
 * Generate creates, updates and destroys goals each knowledge_update using a rule based approach.
 * 
 * Clear rules:
 *  * hear 3 footstep event within 5 seconds then generate a investigation goal.
 *  * I can only use high cover since I'm a large character.
 * 
 * It might need to updated goals:
 *  * Reaction goal for footsteps is updated with explosion sound.
 * 
 * Goals might be time sensitive or get invalidated:
 *  * Invesigation goal created due to footsteps might time out after X seconds.
 *  * I can no longer see my target.
 * 
 * Might get converted into other goals:
 *   * I can't see my target anymore then create search goal.
 * 
 * No decisions in generators, for example:
 * The cover goal should defne max distance so decision can filter... not "the distance".
 */
void goal_generator()
{
}

// several goal generators per archetype and goals can have settings
// ambient, reaction, investigation, combat


/** Determines what a agent should do?
 * 
 * A simple binary broadphase selection, don't select behaviours where preconditions aren't met.
 */
void behavior_selection(Agent* agent)
{
    // this doesn't consider behaviors that can't be interruptible.

    // behavior reselection:
    // behaviors can be infinite... we might want to switch to a better behavior but not a high prio
    // example triggers: by time (evry X seconds), every time a new goal (of specific type) is created/updated

    const auto status = agent->active_behavior->update();

    std::vector<Behavior*> considered_behaviors;
    agent->high_priority.append_behaviors(&considered_behaviors);

    if(status != ExecutionState::running)
    {
        agent->behaviors.append_behaviors(&considered_behaviors);
    }

    if(considered_behaviors.empty()) { return; }

    
    Behavior* new_behavior = [&]()
    {
        if(considered_behaviors.empty())
        {
            return agent->default_behavior;
        }
        else if(considered_behaviors.size() == 1)
        {
            return considered_behaviors[0];
        }
        else
        {
            // select random from top behavior
            return nullptr;
        }
    }();

    // stop current behavior (if needed)
    // start new behavior
}

// can add/remove behaviors during runtime

// each behavior/group can be referenced with a path
// can inject new behavior into a existing group

// package a set of modifications, modifications also can change properties of the agent (add/remove tags so more options are available...)

// can stack modifications as "archetypes"
// example: core ai, basic ranged combat, investigation, elite ranged combat

// tool to test stacking packages + imgui tool to visualize behavior selection for an agent

// example usages:
//  simpler archetypes derivation (basic, veteran, elite...)
//  role assignment in multi role situation (waiter, shopkeeper...)
//  boss buff
//  scripted encounter adjustments

/**@}*/
